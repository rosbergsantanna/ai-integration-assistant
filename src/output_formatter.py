#!/usr/bin/env python3
"""
Output Formatter - AIè¾“å‡ºæ ¼å¼åŒ–Agent
è´Ÿè´£å°†å¤šä¸ªAIæœåŠ¡çš„è¾“å‡ºç»Ÿä¸€æ ¼å¼åŒ–ä¸ºæŒ‡å®šæ ·å¼
"""

import json
import re
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from ai_service_manager import AIResponse


@dataclass
class FormattedOutput:
    """æ ¼å¼åŒ–è¾“å‡ºæ•°æ®ç±»"""
    content: str
    format_type: str
    metadata: Dict[str, Any]


class OutputFormatter:
    """è¾“å‡ºæ ¼å¼åŒ–å™¨"""
    
    def __init__(self, style_config_path: str = None):
        """
        åˆå§‹åŒ–è¾“å‡ºæ ¼å¼åŒ–å™¨
        
        Args:
            style_config_path: è¾“å‡ºæ ·å¼é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.style_config_path = style_config_path or ".claude/output-styles/AIæ•´åˆåŠ©æ‰‹.json"
        self.style_config = self._load_style_config()
    
    def _load_style_config(self) -> Dict:
        """åŠ è½½è¾“å‡ºæ ·å¼é…ç½®"""
        try:
            with open(self.style_config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            # è¿”å›é»˜è®¤é…ç½®
            return {
                "settings": {
                    "output_format": {
                        "ai_output_prefix": "[{ai_name}]: ",
                        "use_tables": True,
                        "section_headers": True,
                        "code_blocks": True
                    }
                },
                "templates": {
                    "analysis_output": "| AIæœåŠ¡ | æ¨¡å‹ | åˆ†æç»“æœ | ç½®ä¿¡åº¦ |\n|--------|------|----------|--------|\n| {ai_name} | {model} | {analysis} | {confidence} |",
                    "ai_response": "[{ai_name}]: {response}",
                    "summary_format": "## ç»¼åˆåˆ†æ\n\n{combined_analysis}\n\n## å»ºè®®æ“ä½œ\n\n{recommendations}"
                }
            }
    
    def format_single_ai_response(self, response: AIResponse) -> str:
        """
        æ ¼å¼åŒ–å•ä¸ªAIå“åº”
        
        Args:
            response: AIå“åº”å¯¹è±¡
            
        Returns:
            str: æ ¼å¼åŒ–åçš„è¾“å‡º
        """
        template = self.style_config.get("templates", {}).get("ai_response", "[{ai_name}]: {response}")
        
        if not response.success:
            return template.format(
                ai_name=response.service_name,
                response=f"è°ƒç”¨å¤±è´¥: {response.error_message}"
            )
        
        # å¤„ç†è¾“å‡ºå†…å®¹
        content = self._clean_content(response.content)
        
        # æ·»åŠ ç½®ä¿¡åº¦å’Œå“åº”æ—¶é—´ä¿¡æ¯
        metadata = f" (ç½®ä¿¡åº¦: {response.confidence:.1f}/10, å“åº”æ—¶é—´: {response.response_time:.2f}s)"
        
        return template.format(
            ai_name=response.service_name,
            response=content + metadata
        )
    
    def format_analysis_table(self, responses: List[AIResponse]) -> str:
        """
        æ ¼å¼åŒ–ä¸ºåˆ†æè¡¨æ ¼
        
        Args:
            responses: AIå“åº”åˆ—è¡¨
            
        Returns:
            str: è¡¨æ ¼æ ¼å¼çš„åˆ†æç»“æœ
        """
        if not responses:
            return "æ— AIåˆ†ææ•°æ®"
        
        # è¡¨æ ¼å¤´éƒ¨
        table_lines = [
            "| AIæœåŠ¡ | æ¨¡å‹ | çŠ¶æ€ | åˆ†æç»“æœé¢„è§ˆ | ç½®ä¿¡åº¦ | å“åº”æ—¶é—´ |",
            "|--------|------|------|-------------|--------|----------|"
        ]
        
        for response in responses:
            service_name = self._get_service_display_name(response.service_name)
            model_name = response.model_name
            
            if response.success:
                status = "âœ… æˆåŠŸ"
                preview = self._truncate_content(response.content, 50)
                confidence = f"{response.confidence:.1f}/10"
            else:
                status = "âŒ å¤±è´¥"
                preview = response.error_message or "æœªçŸ¥é”™è¯¯"
                confidence = "0/10"
            
            response_time = f"{response.response_time:.2f}s"
            
            table_lines.append(
                f"| {service_name} | {model_name} | {status} | {preview} | {confidence} | {response_time} |"
            )
        
        return "\n".join(table_lines)
    
    def format_detailed_responses(self, responses: List[AIResponse]) -> str:
        """
        æ ¼å¼åŒ–è¯¦ç»†å“åº”å†…å®¹
        
        Args:
            responses: AIå“åº”åˆ—è¡¨
            
        Returns:
            str: è¯¦ç»†æ ¼å¼åŒ–çš„å“åº”å†…å®¹
        """
        if not responses:
            return "æ— AIå“åº”æ•°æ®"
        
        sections = []
        
        for i, response in enumerate(responses, 1):
            service_display = self._get_service_display_name(response.service_name)
            
            if response.success:
                section = f"""### {i}. {service_display} ({response.model_name})

**çŠ¶æ€**: âœ… è°ƒç”¨æˆåŠŸ
**ç½®ä¿¡åº¦**: {response.confidence:.1f}/10
**å“åº”æ—¶é—´**: {response.response_time:.2f}s
**Tokenä½¿ç”¨**: {self._format_token_usage(response.token_usage)}

**åˆ†æå†…å®¹**:
{self._format_content_with_blocks(response.content)}"""
            else:
                section = f"""### {i}. {service_display} ({response.model_name})

**çŠ¶æ€**: âŒ è°ƒç”¨å¤±è´¥
**é”™è¯¯ä¿¡æ¯**: {response.error_message}
**å“åº”æ—¶é—´**: {response.response_time:.2f}s"""
            
            sections.append(section)
        
        return "\n\n---\n\n".join(sections)
    
    def format_combined_analysis(self, responses: List[AIResponse]) -> str:
        """
        æ ¼å¼åŒ–ç»¼åˆåˆ†æç»“æœ
        
        Args:
            responses: AIå“åº”åˆ—è¡¨
            
        Returns:
            str: ç»¼åˆåˆ†ææ ¼å¼åŒ–è¾“å‡º
        """
        successful_responses = [r for r in responses if r.success]
        failed_responses = [r for r in responses if not r.success]
        
        if not successful_responses:
            return "## âš ï¸ åˆ†æå¤±è´¥\n\næ‰€æœ‰AIæœåŠ¡è°ƒç”¨å‡å¤±è´¥ï¼Œæ— æ³•æä¾›åˆ†æç»“æœã€‚"
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_services = len(responses)
        success_count = len(successful_responses)
        avg_confidence = sum(r.confidence for r in successful_responses) / len(successful_responses)
        avg_response_time = sum(r.response_time for r in successful_responses) / len(successful_responses)
        
        # æ„å»ºç»¼åˆåˆ†æ
        analysis_parts = []
        
        # å¤´éƒ¨ç»Ÿè®¡
        stats_section = f"""## ğŸ“Š åˆ†æç»Ÿè®¡

| æŒ‡æ ‡ | å€¼ |
|------|-----|
| è°ƒç”¨æœåŠ¡æ•° | {total_services} |
| æˆåŠŸè°ƒç”¨æ•° | {success_count} |
| æˆåŠŸç‡ | {(success_count/total_services)*100:.1f}% |
| å¹³å‡ç½®ä¿¡åº¦ | {avg_confidence:.1f}/10 |
| å¹³å‡å“åº”æ—¶é—´ | {avg_response_time:.2f}s |

"""
        analysis_parts.append(stats_section)
        
        # å¿«é€Ÿæ¦‚è§ˆè¡¨æ ¼
        if self.style_config.get("settings", {}).get("output_format", {}).get("use_tables", True):
            table_section = "## ğŸ“‹ åˆ†ææ¦‚è§ˆ\n\n" + self.format_analysis_table(responses) + "\n\n"
            analysis_parts.append(table_section)
        
        # æˆåŠŸçš„åˆ†æç»“æœ
        if successful_responses:
            success_section = "## âœ… æˆåŠŸåˆ†æç»“æœ\n\n"
            for response in successful_responses:
                service_name = self._get_service_display_name(response.service_name)
                success_section += f"**[{service_name}]**: {self._truncate_content(response.content, 200)}\n\n"
            analysis_parts.append(success_section)
        
        # å¤±è´¥çš„è°ƒç”¨ä¿¡æ¯
        if failed_responses:
            fail_section = "## âŒ å¤±è´¥è°ƒç”¨ä¿¡æ¯\n\n"
            for response in failed_responses:
                service_name = self._get_service_display_name(response.service_name)
                fail_section += f"**[{service_name}]**: {response.error_message}\n\n"
            analysis_parts.append(fail_section)
        
        # ç»¼åˆå»ºè®®
        if len(successful_responses) >= 2:
            recommendations = self._generate_recommendations(successful_responses)
            if recommendations:
                rec_section = f"## ğŸ¯ ç»¼åˆå»ºè®®\n\n{recommendations}\n\n"
                analysis_parts.append(rec_section)
        
        return "".join(analysis_parts)
    
    def format_for_claude_code(self, responses: List[AIResponse], format_type: str = "combined") -> FormattedOutput:
        """
        æ ¼å¼åŒ–ä¸ºClaude Codeé€‚ç”¨çš„è¾“å‡º
        
        Args:
            responses: AIå“åº”åˆ—è¡¨
            format_type: æ ¼å¼ç±»å‹ (table, detailed, combined)
            
        Returns:
            FormattedOutput: æ ¼å¼åŒ–è¾“å‡ºå¯¹è±¡
        """
        if format_type == "table":
            content = self.format_analysis_table(responses)
        elif format_type == "detailed":
            content = self.format_detailed_responses(responses)
        else:  # combined
            content = self.format_combined_analysis(responses)
        
        metadata = {
            "total_responses": len(responses),
            "successful_responses": len([r for r in responses if r.success]),
            "format_type": format_type,
            "timestamp": self._get_timestamp()
        }
        
        return FormattedOutput(
            content=content,
            format_type=format_type,
            metadata=metadata
        )
    
    def _get_service_display_name(self, service_name: str) -> str:
        """è·å–æœåŠ¡æ˜¾ç¤ºåç§°"""
        name_mapping = {
            "zhipu": "æ™ºè°±è½»è¨€",
            "silicon": "ç¡…åŸºæµåŠ¨",
            "openai": "OpenAI",
            "claude": "Claude"
        }
        return name_mapping.get(service_name, service_name.title())
    
    def _clean_content(self, content: str) -> str:
        """æ¸…ç†å†…å®¹æ ¼å¼"""
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        content = re.sub(r'\n\s*\n', '\n\n', content.strip())
        # å¤„ç†è¡¨æ ¼å­—ç¬¦è½¬ä¹‰
        content = content.replace('|', '\\|')
        return content
    
    def _truncate_content(self, content: str, max_length: int) -> str:
        """æˆªæ–­å†…å®¹"""
        if len(content) <= max_length:
            return content.replace('\n', ' ')
        
        truncated = content[:max_length].replace('\n', ' ')
        return truncated + "..."
    
    def _format_content_with_blocks(self, content: str) -> str:
        """æ ¼å¼åŒ–å†…å®¹ï¼ŒåŒ…å«ä»£ç å—å¤„ç†"""
        # æ£€æµ‹æ˜¯å¦åŒ…å«ä»£ç 
        if '```' in content or content.count('\n') > 10:
            return f"```\n{content}\n```"
        return content
    
    def _format_token_usage(self, token_usage: Dict[str, int]) -> str:
        """æ ¼å¼åŒ–tokenä½¿ç”¨ä¿¡æ¯"""
        if not token_usage:
            return "æœªçŸ¥"
        
        total = token_usage.get('total_tokens', 0)
        prompt = token_usage.get('prompt_tokens', 0)
        completion = token_usage.get('completion_tokens', 0)
        
        if total:
            return f"æ€»è®¡: {total}, è¾“å…¥: {prompt}, è¾“å‡º: {completion}"
        
        return "æœªè®°å½•"
    
    def _generate_recommendations(self, responses: List[AIResponse]) -> str:
        """ç”Ÿæˆç»¼åˆå»ºè®®"""
        if len(responses) < 2:
            return ""
        
        # ç®€å•çš„å»ºè®®ç”Ÿæˆé€»è¾‘
        high_confidence = [r for r in responses if r.confidence >= 8.0]
        medium_confidence = [r for r in responses if 6.0 <= r.confidence < 8.0]
        
        recommendations = []
        
        if high_confidence:
            rec = f"- é«˜ç½®ä¿¡åº¦åˆ†æ ({len(high_confidence)}ä¸ª): å»ºè®®ä¼˜å…ˆé‡‡çº³è¿™äº›å»ºè®®"
            recommendations.append(rec)
        
        if medium_confidence:
            rec = f"- ä¸­ç­‰ç½®ä¿¡åº¦åˆ†æ ({len(medium_confidence)}ä¸ª): å¯ä½œä¸ºå‚è€ƒè¡¥å……"
            recommendations.append(rec)
        
        if len(set(r.service_name for r in responses)) >= 2:
            recommendations.append("- å¤šæœåŠ¡éªŒè¯: å»ºè®®ç»“åˆå¤šä¸ªAIæ„è§åšæœ€ç»ˆå†³ç­–")
        
        fast_responses = [r for r in responses if r.response_time < 2.0]
        if fast_responses:
            recommendations.append(f"- å¿«é€Ÿå“åº”æœåŠ¡ ({len(fast_responses)}ä¸ª): é€‚åˆåç»­å¿«é€Ÿè¿­ä»£ä½¿ç”¨")
        
        return "\n".join(recommendations) if recommendations else ""
    
    def _get_timestamp(self) -> str:
        """è·å–æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


class AIIntegrationAgent:
    """AIæ•´åˆåŠ©æ‰‹Agent - ç»“åˆå¤šä¸ªAIæœåŠ¡è¿›è¡Œåä½œåˆ†æ"""
    
    def __init__(self, config_path: str = None, style_path: str = None):
        """
        åˆå§‹åŒ–AIæ•´åˆåŠ©æ‰‹
        
        Args:
            config_path: AIæœåŠ¡é…ç½®æ–‡ä»¶è·¯å¾„
            style_path: è¾“å‡ºæ ·å¼é…ç½®æ–‡ä»¶è·¯å¾„
        """
        from ai_service_manager import AIServiceManager
        
        self.service_manager = AIServiceManager(config_path)
        self.formatter = OutputFormatter(style_path)
        self.session_active = False
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self.service_manager.__aenter__()
        self.session_active = True
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        await self.service_manager.__aexit__(exc_type, exc_val, exc_tb)
        self.session_active = False
    
    async def analyze_code(self, code: str, language: str = "python") -> FormattedOutput:
        """
        ä»£ç åˆ†æ
        
        Args:
            code: è¦åˆ†æçš„ä»£ç 
            language: ç¼–ç¨‹è¯­è¨€
            
        Returns:
            FormattedOutput: æ ¼å¼åŒ–çš„åˆ†æç»“æœ
        """
        prompt = f"""è¯·å¯¹ä»¥ä¸‹{language}ä»£ç è¿›è¡Œå…¨é¢åˆ†æï¼š

```{language}
{code}
```

è¯·ä»ä»¥ä¸‹è§’åº¦è¿›è¡Œåˆ†æï¼š
1. ä»£ç è´¨é‡å’Œè§„èŒƒæ€§
2. æ€§èƒ½ä¼˜åŒ–å»ºè®®
3. å®‰å…¨æ€§æ£€æŸ¥
4. å¯ç»´æŠ¤æ€§è¯„ä¼°
5. æ½œåœ¨é—®é¢˜è¯†åˆ«

è¯·æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®å’Œæœ€ä½³å®è·µæ¨èã€‚"""
        
        responses = await self.service_manager.analyze_with_multiple_ai(prompt, template_type="code_review")
        return self.formatter.format_for_claude_code(responses, "combined")
    
    async def analyze_error(self, error_message: str, code: str = "", language: str = "python") -> FormattedOutput:
        """
        é”™è¯¯åˆ†æ
        
        Args:
            error_message: é”™è¯¯æ¶ˆæ¯
            code: ç›¸å…³ä»£ç 
            language: ç¼–ç¨‹è¯­è¨€
            
        Returns:
            FormattedOutput: æ ¼å¼åŒ–çš„åˆ†æç»“æœ
        """
        prompt = f"""è¯·åˆ†æä»¥ä¸‹é”™è¯¯ï¼š

é”™è¯¯ä¿¡æ¯ï¼š
{error_message}

ç›¸å…³ä»£ç ï¼š
```{language}
{code}
```

è¯·æä¾›ï¼š
1. é”™è¯¯æ ¹æœ¬åŸå› åˆ†æ
2. å…·ä½“è§£å†³æ–¹æ¡ˆ
3. é¢„é˜²æªæ–½å»ºè®®
4. ç›¸å…³æœ€ä½³å®è·µ"""
        
        responses = await self.service_manager.analyze_with_multiple_ai(prompt, template_type="bug_analysis")
        return self.formatter.format_for_claude_code(responses, "combined")
    
    async def general_analysis(self, content: str, analysis_type: str = "general") -> FormattedOutput:
        """
        é€šç”¨åˆ†æ
        
        Args:
            content: è¦åˆ†æçš„å†…å®¹
            analysis_type: åˆ†æç±»å‹
            
        Returns:
            FormattedOutput: æ ¼å¼åŒ–çš„åˆ†æç»“æœ
        """
        responses = await self.service_manager.analyze_with_multiple_ai(content, template_type="analysis")
        return self.formatter.format_for_claude_code(responses, "combined")
    
    def get_service_status(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡çŠ¶æ€"""
        available_services = self.service_manager.get_available_services()
        free_models = self.service_manager.get_free_models()
        
        return {
            "available_services": available_services,
            "free_models": free_models,
            "session_active": self.session_active,
            "total_services": len(self.service_manager.config['services']),
            "enabled_services": len(available_services)
        }


if __name__ == "__main__":
    # æµ‹è¯•ç¤ºä¾‹
    import asyncio
    
    async def test_formatter():
        """æµ‹è¯•è¾“å‡ºæ ¼å¼åŒ–å™¨"""
        # åˆ›å»ºæ¨¡æ‹Ÿå“åº”
        responses = [
            AIResponse(
                service_name="zhipu",
                model_name="glm-4-flash",
                content="è¿™æ®µä»£ç æ•´ä½“ç»“æ„æ¸…æ™°ï¼Œä½†å­˜åœ¨ä¸€äº›å¯ä»¥ä¼˜åŒ–çš„åœ°æ–¹...",
                confidence=8.5,
                token_usage={"total_tokens": 150, "prompt_tokens": 100, "completion_tokens": 50},
                response_time=1.2,
                success=True
            ),
            AIResponse(
                service_name="silicon",
                model_name="deepseek-v2.5",
                content="ä»£ç è´¨é‡è‰¯å¥½ï¼Œå»ºè®®æ·»åŠ æ›´å¤šçš„é”™è¯¯å¤„ç†...",
                confidence=7.8,
                token_usage={"total_tokens": 200, "prompt_tokens": 120, "completion_tokens": 80},
                response_time=2.1,
                success=True
            )
        ]
        
        formatter = OutputFormatter()
        result = formatter.format_for_claude_code(responses, "combined")
        print(result.content)
    
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        import sys
        asyncio.run(test_formatter())
    else:
        print("Output Formatter - AIè¾“å‡ºæ ¼å¼åŒ–Agent")
        print("ä½¿ç”¨ 'python output_formatter.py test' è¿è¡Œæµ‹è¯•")