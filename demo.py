#!/usr/bin/env python3
"""
AIæ•´åˆåŠ©æ‰‹æ¼”ç¤ºè„šæœ¬
å±•ç¤ºå¦‚ä½•åœ¨ä»£ç ä¸­ç›´æ¥ä½¿ç”¨AIæ•´åˆåŠ©æ‰‹åŠŸèƒ½
"""

import asyncio
import sys
import os

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.ai_service_manager import AIServiceManager
from src.output_formatter import AIIntegrationAgent, OutputFormatter


async def demo_basic_usage():
    """åŸºæœ¬ä½¿ç”¨æ¼”ç¤º"""
    print("ğŸš€ AIæ•´åˆåŠ©æ‰‹åŸºæœ¬ä½¿ç”¨æ¼”ç¤º")
    print("=" * 50)
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    config_path = ".claude/ai-services-config.json"
    if not os.path.exists(config_path):
        print("âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ:")
        print("   python ai_assistant_cli.py init")
        print("   python ai_assistant_cli.py config zhipu <your-api-key>")
        return
    
    try:
        # åˆ›å»ºAIæ•´åˆåŠ©æ‰‹å®ä¾‹
        async with AIIntegrationAgent() as agent:
            
            # è·å–æœåŠ¡çŠ¶æ€
            status = agent.get_service_status()
            print(f"ğŸ“Š æœåŠ¡çŠ¶æ€:")
            print(f"   å¯ç”¨æœåŠ¡: {len(status['available_services'])}/{status['total_services']}")
            print(f"   æœåŠ¡åˆ—è¡¨: {', '.join(status['available_services'])}")
            
            if not status['available_services']:
                print("âŒ æ²¡æœ‰å¯ç”¨çš„AIæœåŠ¡ï¼Œè¯·å…ˆé…ç½®")
                print("   python ai_assistant_cli.py config zhipu <your-api-key>")
                return
            
            print(f"   å…è´¹æ¨¡å‹: {status['free_models']}")
            print()
            
            # æ¼”ç¤º1: é€šç”¨æ–‡æœ¬åˆ†æ
            print("ğŸ“ æ¼”ç¤º1: é€šç”¨æ–‡æœ¬åˆ†æ")
            print("-" * 30)
            
            sample_text = """
            Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶ç®€æ´æ˜“è¯»çš„è¯­æ³•è€Œé—»åã€‚
            å®ƒå¹¿æ³›åº”ç”¨äºWebå¼€å‘ã€æ•°æ®ç§‘å­¦ã€äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸã€‚
            Pythonçš„å“²å­¦æ˜¯"ä¼˜é›…"ã€"æ˜ç¡®"ã€"ç®€å•"ã€‚
            """
            
            print(f"åˆ†æå†…å®¹: {sample_text.strip()}")
            print("\nğŸ” å¼€å§‹åˆ†æ...")
            
            result = await agent.general_analysis(sample_text)
            print(result.content)
            print("\n" + "=" * 50 + "\n")
            
            # æ¼”ç¤º2: ä»£ç åˆ†æ
            print("ğŸ”§ æ¼”ç¤º2: ä»£ç åˆ†æ")
            print("-" * 30)
            
            sample_code = '''
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

result = fibonacci(10)
print(result)
            '''
            
            print("åˆ†æä»£ç :")
            print(sample_code)
            print("ğŸ” å¼€å§‹ä»£ç åˆ†æ...")
            
            result = await agent.analyze_code(sample_code, "python")
            print(result.content)
            print("\n" + "=" * 50 + "\n")
            
            # æ¼”ç¤º3: é”™è¯¯åˆ†æ
            print("ğŸ› æ¼”ç¤º3: é”™è¯¯åˆ†æ")
            print("-" * 30)
            
            error_msg = "IndexError: list index out of range"
            error_code = '''
data = [1, 2, 3]
for i in range(5):
    print(data[i])  # è¿™é‡Œä¼šå‡ºé”™
            '''
            
            print(f"é”™è¯¯ä¿¡æ¯: {error_msg}")
            print("ç›¸å…³ä»£ç :")
            print(error_code)
            print("ğŸ” å¼€å§‹é”™è¯¯åˆ†æ...")
            
            result = await agent.analyze_error(error_msg, error_code, "python")
            print(result.content)
            
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


async def demo_output_formats():
    """è¾“å‡ºæ ¼å¼æ¼”ç¤º"""
    print("\nğŸ¨ è¾“å‡ºæ ¼å¼æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºæ¨¡æ‹Ÿå“åº”æ•°æ®
    from src.ai_service_manager import AIResponse
    
    mock_responses = [
        AIResponse(
            service_name="zhipu",
            model_name="glm-4-flash", 
            content="è¿™æ˜¯æ™ºè°±è½»è¨€çš„åˆ†æç»“æœã€‚ä»£ç æ•´ä½“ç»“æ„è‰¯å¥½ï¼Œä½†å»ºè®®æ·»åŠ é”™è¯¯å¤„ç†æœºåˆ¶ä»¥æé«˜å¥å£®æ€§ã€‚",
            confidence=8.5,
            token_usage={"total_tokens": 120, "prompt_tokens": 80, "completion_tokens": 40},
            response_time=1.2,
            success=True
        ),
        AIResponse(
            service_name="silicon",
            model_name="deepseek-v2.5",
            content="ä»æ€§èƒ½è§’åº¦åˆ†æï¼Œå½“å‰å®ç°å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚å»ºè®®ä½¿ç”¨ç¼“å­˜æœºåˆ¶å‡å°‘é‡å¤è®¡ç®—ã€‚",
            confidence=7.8,
            token_usage={"total_tokens": 150, "prompt_tokens": 90, "completion_tokens": 60},
            response_time=2.1,
            success=True
        ),
        AIResponse(
            service_name="openai",
            model_name="gpt-3.5-turbo",
            content="",
            confidence=0.0,
            token_usage={},
            response_time=0.5,
            success=False,
            error_message="APIå¯†é’¥æœªé…ç½®"
        )
    ]
    
    formatter = OutputFormatter()
    
    # æ¼”ç¤ºä¸åŒçš„è¾“å‡ºæ ¼å¼
    print("ğŸ“‹ è¡¨æ ¼æ ¼å¼:")
    table_result = formatter.format_for_claude_code(mock_responses, "table")
    print(table_result.content)
    print()
    
    print("ğŸ“– è¯¦ç»†æ ¼å¼:")
    detailed_result = formatter.format_for_claude_code(mock_responses, "detailed") 
    print(detailed_result.content)
    print()
    
    print("ğŸ”„ ç»¼åˆæ ¼å¼:")
    combined_result = formatter.format_for_claude_code(mock_responses, "combined")
    print(combined_result.content)


async def demo_service_manager():
    """AIæœåŠ¡ç®¡ç†å™¨æ¼”ç¤º"""
    print("\nğŸ¤– AIæœåŠ¡ç®¡ç†å™¨æ¼”ç¤º")  
    print("=" * 50)
    
    try:
        async with AIServiceManager() as manager:
            
            # è·å–æœåŠ¡ä¿¡æ¯
            available = manager.get_available_services()
            all_models = {}
            free_models = manager.get_free_models()
            
            print(f"ğŸ“¡ å¯ç”¨æœåŠ¡: {available}")
            print(f"ğŸ†“ å…è´¹æ¨¡å‹: {free_models}")
            
            for service in available:
                models = manager.get_service_models(service)
                all_models[service] = models
                print(f"   {service}: {models}")
            
            if not available:
                print("âŒ æ²¡æœ‰å¯ç”¨æœåŠ¡ï¼Œæ¼”ç¤ºç»“æŸ")
                return
            
            # æµ‹è¯•å•ä¸ªæœåŠ¡è°ƒç”¨
            print(f"\nğŸ§ª æµ‹è¯•æœåŠ¡è°ƒç”¨:")
            service_name = available[0]
            model_name = all_models[service_name][0]
            
            print(f"   è°ƒç”¨ {service_name} ({model_name})...")
            
            response = await manager.call_ai_service(
                service_name, 
                model_name, 
                "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹"
            )
            
            if response.success:
                print(f"   âœ… è°ƒç”¨æˆåŠŸ")
                print(f"   ğŸ“ å“åº”å†…å®¹: {response.content[:100]}...")
                print(f"   â±ï¸  å“åº”æ—¶é—´: {response.response_time:.2f}s")
                print(f"   ğŸ¯ ç½®ä¿¡åº¦: {response.confidence}/10")
            else:
                print(f"   âŒ è°ƒç”¨å¤±è´¥: {response.error_message}")
            
    except Exception as e:
        print(f"âŒ æœåŠ¡ç®¡ç†å™¨æ¼”ç¤ºå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸª AIæ•´åˆåŠ©æ‰‹å®Œæ•´æ¼”ç¤º")
    print("é€‚ç”¨äºClaude Codeç¯å¢ƒçš„å¤šAIæœåŠ¡æ•´åˆå·¥å…·")
    print("=" * 60)
    
    try:
        # è¿è¡Œæ‰€æœ‰æ¼”ç¤º
        asyncio.run(demo_basic_usage())
        asyncio.run(demo_output_formats())
        asyncio.run(demo_service_manager())
        
        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ“š æ›´å¤šç”¨æ³•:")
        print("   python ai_assistant_cli.py --help")
        print("   python ai_assistant_cli.py list")
        print("   python ai_assistant_cli.py analyze 'ä½ çš„å†…å®¹'")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºå¤±è´¥: {e}")


if __name__ == "__main__":
    main()